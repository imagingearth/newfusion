# swath_download.R
# Version 1.1
# Tools
#
# Project: New Fusion
# By xjtang
# Created On: 11/9/2014
# Last Update: 2/20/2015
#
# Input Arguments: 
#   file (String) - a text file that is generated by LAADS website.
#   output (String) - the output directory for saving the data.
#   njob (Integer) - total number of jobs submitted (1 indicates single job).
#   ijob (Integer) - the number of current job.
#   
# Output Arguments: NA
#
# Instruction: 
#   1.Install package RCurl and bitops.
#   2.Search for swath data on the LAADS website.
#   3.Submit your order.
#   3.Save the order page with all the files to be downloaded as a text file.
#   4.Run the script with the text file after the order is processed.
#
# Version 1.0 - 11/9/2014
#   This script downloads MODIS Swath data
#
# Updates of Version 1.0.1 - 11/12/2014
#   1.Bug fixed.
#
# Updates of Version 1.1 - 2/20/2015
#   1.Added support for multiple jobs.
#   
# Created on Github on 11/4/2014, check Github Commits for updates afterwards.
#----------------------------------------------------------------

# library and sourcing
library(RCurl)
script <- getURL('https://raw.githubusercontent.com/xjtang/rTools/master/web_tools.R',ssl.verifypeer=F)
eval(parse(text=script),envir=.GlobalEnv)
sourceURL('https://raw.githubusercontent.com/xjtang/rTools/master/text_tools.R')
rm(script)

#--------------------------------------

# main function
swathDownload <- function(webFile,output,njob=1,ijob=1){

  # set base url of usgs ftp
  baseURL <- 'ftp://ladsweb.nascom.nasa.gov/allData'

  # check input html file
  if(!file.exists(webFile)){
    cat('Can not find input HTML file:\n')
    cat(webFile,'\n')
    cat('Please check again.\n')
    return(-1)
  }

  # remove the trailing / from the output directory
  if(strRight(output,1) == '/'){
    output <- trimRight(output,1)
  }

  # check if output directory exist
  if(!file.exists(output)){
    cat('Output directory does not exist, creating one.\n')
    dir.create(output)
  }

  # check if njob ijob is valid
  if(njob<1){
    cat('Invalid number of jobs.\n')
    return(-2)
  }
  if((njob>1)&(ijob>njob)){
    cat('Invalid number of current job.\n')
    return(-3)
  }

  # read in html file
  imgList <- readLines(webFile,warn=F)
  
  # pre-process list
  pat <- '^.*(M.*D09.*hdf).*$'
  imgList <- gsub(pat, '\\1', imgList[grepl(pat,imgList)])

  # subset the list to current job
  if(njob>1){
    # calculate begining and ending
    total <- length(imgList)
    piece <- floor(total/njob)
    start <- 1+piece*(ijob-1)
    if(ijob<njob){
        stop <- start+piece-1
    }else{
        stop <- total
    }
    # subset dates to be processed
    imgList <- imgList[start:stop]
  }

  # count numbers 
  n <- length(imgList)

  # loop though the list
  for(i in 1:n){
  
    # progress
    pgs <- paste('(',i,'/',n,')',sep='')
    
    # parse the file name
    img <- unlist(strsplit(imgList[i],'.',fixed=T))
    col <- as.integer(img[4])
    prod <- img[1]
    year <- substr(img[2],2,5)
    day <- substr(img[2],6,9)
    
    # forge url and output file
    fullURL <- paste(baseURL,col,prod,year,day,imgList[i],sep='/')
    outFile <- paste(output,imgList[i],sep='/')
    
    # check if file exist on ftp
    if(!url.exists(fullURL)){
      cat(pgs,'File does not exist on the ftp: ',fullURL)
      cat('\nSkip this one\n')
      next
    }
    
    # check if file already exist in output
    if(file.exists(outFile)){
      cat(pgs,'File already exist in output directory: ',outFile)
      cat('\nSkip this one\n')
      next
    }
    
    # download
    cat(pgs,'Downloading from: ',fullURL)
    try(binDownload(fullURL,outFile))
    cat('...done\n')

    # garbage collection
    gc()

  }

  # done
  cat('all done\n')

}

#--------------------------------------
